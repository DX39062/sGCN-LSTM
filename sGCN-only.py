import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, balanced_accuracy_score
import pandas as pd
import numpy as np

# ==========================================
# ç¬¬ä¸€éƒ¨åˆ†: æ•°æ®åŠ è½½
# ==========================================

def load_adjacency_matrix(adj_file, n_nodes=116):
    print(f"--- æ­£åœ¨åŠ è½½é‚»æ¥çŸ©é˜µ: {adj_file} ---")
    try:
        adj_np = pd.read_csv(adj_file, header=None).values.astype(np.float32)
        if adj_np.shape != (n_nodes, n_nodes):
            raise ValueError(f"é‚»æ¥çŸ©é˜µå½¢çŠ¶é”™è¯¯: æœŸæœ› ({n_nodes}, {n_nodes}), å®é™… {adj_np.shape}")
        
        np.fill_diagonal(adj_np, 0)
        adj_torch = torch.tensor(adj_np)
        
        A_tilde = adj_torch + torch.eye(n_nodes)
        degrees = torch.sum(A_tilde, dim=1)
        D_inv_sqrt = torch.pow(degrees, -0.5)
        D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0
        D_mat = torch.diag(D_inv_sqrt)
        adj_normalized = D_mat @ A_tilde @ D_mat
        
        print(" é‚»æ¥çŸ©é˜µåŠ è½½å¹¶å½’ä¸€åŒ–å®Œæˆã€‚")
        return adj_normalized
    except Exception as e:
        print(f"åŠ è½½é‚»æ¥çŸ©é˜µæ—¶å‡ºé”™: {e}")
        raise e

class MultimodalDataset(Dataset):
    def __init__(self, fmri_dir, smri_file, label_file, n_time_steps=140, n_nodes=116):
        self.fmri_dir = fmri_dir
        self.n_time_steps = n_time_steps
        self.n_nodes = n_nodes
        
        # 1. è§£ææ ‡ç­¾
        self.label_map = {}
        try:
            with open(label_file, 'r') as f:
                lines = f.readlines()
                for line in lines:
                    if ':' in line:
                        parts = line.strip().split(':')
                        clean_id = parts[0].strip().replace("'", "").replace('"', "")
                        clean_label = int(parts[1].strip().replace(",", ""))
                        self.label_map[clean_id] = clean_label
        except Exception as e:
            print(f"è§£ææ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}")
            raise e
            
        # 2. åŠ è½½ sMRI
        if not os.path.exists(smri_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° sMRI æ–‡ä»¶: {smri_file}")
            
        self.smri_df = pd.read_csv(smri_file)
        if 'Subject_ID' in self.smri_df.columns:
            self.smri_df = self.smri_df.set_index('Subject_ID')
        else:
            self.smri_df = self.smri_df.set_index(self.smri_df.columns[0])

        # 3. åŒ¹é…æ•°æ®
        self.data_list = [] 
        self.labels_list = [] 
        
        if not os.path.exists(fmri_dir):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° fMRI æ–‡ä»¶å¤¹: {fmri_dir}")
            
        fmri_files = sorted([f for f in os.listdir(fmri_dir) if f.endswith('.csv')])
        
        match_count = 0
        for f_file in fmri_files:
            long_id = os.path.splitext(f_file)[0]
            parts = long_id.split('_')
            if len(parts) >= 3:
                short_id = "_".join(parts[:3])
            else:
                short_id = long_id
            
            if short_id in self.label_map and long_id in self.smri_df.index:
                label = self.label_map[short_id]
                f_path = os.path.join(fmri_dir, f_file)
                self.data_list.append((f_path, long_id, label))
                self.labels_list.append(label)
                match_count += 1
                
        print(f" æ•°æ®é›†åˆå§‹åŒ–: æ‰¾åˆ° {match_count} ä¸ªå®Œæ•´æ ·æœ¬ã€‚")
        if match_count == 0:
            raise RuntimeError("æ•°æ®åŒ¹é…å¤±è´¥")

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        f_path, subject_id, label = self.data_list[idx]
        
        try:
            fmri_data = pd.read_csv(f_path, header=None).values.astype(np.float32)
            if fmri_data.shape != (self.n_time_steps, self.n_nodes):
                if fmri_data.shape == (self.n_nodes, self.n_time_steps):
                    fmri_data = fmri_data.T
                elif fmri_data.shape[0] > self.n_time_steps:
                    fmri_data = fmri_data[:self.n_time_steps, :]
                else:
                    padding = np.zeros((self.n_time_steps - fmri_data.shape[0], self.n_nodes))
                    fmri_data = np.vstack([fmri_data, padding])

            scaler = StandardScaler()
            fmri_data = scaler.fit_transform(fmri_data) 

            smri_row = self.smri_df.loc[subject_id].values.astype(np.float32)
            if smri_row.shape[0] != self.n_nodes:
                 smri_row = smri_row[:self.n_nodes]
            
            if np.std(smri_row) > 0:
                smri_row = (smri_row - np.mean(smri_row)) / np.std(smri_row)
            else:
                smri_row = smri_row - np.mean(smri_row)

            return (
                torch.tensor(fmri_data, dtype=torch.float32), 
                torch.tensor(smri_row, dtype=torch.float32), 
                torch.tensor(label, dtype=torch.long)
            )
        except Exception as e:
            print(f"è¯»å–æ•°æ®å‡ºé”™ (ID: {subject_id}): {e}")
            return (torch.zeros(self.n_time_steps, self.n_nodes), 
                    torch.zeros(self.n_nodes), 
                    torch.tensor(0, dtype=torch.long))

# ==========================================
# ç¬¬äºŒéƒ¨åˆ†: ä»…åŒ…å« GCN çš„æ¨¡å‹
# ==========================================

class StructureGatedGCN(nn.Module):
    """
    ä¿æŒåŸæœ‰çš„å¼ºå£®ç»“æ„ (æ®‹å·® + BN + ç»“æ„é—¨æ§)
    """
    def __init__(self, n_nodes=116, feature_dim=64):
        super(StructureGatedGCN, self).__init__()
        self.n_nodes = n_nodes
        
        self.struct_gate = nn.Sequential(
            nn.Linear(1, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        self.fmri_linear = nn.Linear(1, feature_dim)
        self.residual_proj = nn.Linear(1, feature_dim)
        self.dropout = nn.Dropout(0.3)

    def forward(self, fmri, smri_gmv, adj_static):
        # ç»“æ„é—¨æ§
        node_integrity = self.struct_gate(smri_gmv.unsqueeze(-1)) 
        struct_mask = (node_integrity @ node_integrity.transpose(1, 2)) + 0.1
        adj_dynamic = adj_static.unsqueeze(0) * struct_mask
        
        # GCN
        fmri_feat = self.fmri_linear(fmri.unsqueeze(-1)) 
        fmri_feat = F.relu(fmri_feat)
        
        out = torch.einsum('bmn, btnf -> btmf', adj_dynamic, fmri_feat)
        
        # æ®‹å·®è¿æ¥
        residual = self.residual_proj(fmri.unsqueeze(-1))
        out = out + residual 
        
        out = F.relu(out)
        out = self.dropout(out)
        
        # èŠ‚ç‚¹æ± åŒ–: (Batch, Time, Nodes, Feat) -> (Batch, Time, Feat)
        out = torch.mean(out, dim=2) 
        
        return out 

class GCN_Only_Model(nn.Module):
    """
    æ—  LSTM ç‰ˆæœ¬ï¼šç›´æ¥å¯¹æ—¶é—´ç»´åº¦å–å¹³å‡
    """
    def __init__(self, n_nodes=116, n_classes=2):
        super(GCN_Only_Model, self).__init__()
        self.hidden_dim = 64  
        
        # GCN æ¨¡å—
        self.struct_gcn = StructureGatedGCN(n_nodes=n_nodes, feature_dim=self.hidden_dim)
        
        # Batch Normalization (ç›´æ¥å¯¹ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–)
        self.bn_final = nn.BatchNorm1d(self.hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, n_classes)
        )
        
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, fmri, smri, adj_static):
        # 1. GCN æå–ç‰¹å¾
        # è¾“å‡ºå½¢çŠ¶: (Batch, Time, Hidden_Dim)
        gcn_out = self.struct_gcn(fmri, smri, adj_static)
        
        # 2. å…¨å±€æ—¶é—´å¹³å‡æ± åŒ– (Global Average Pooling over Time)
        # å°†æ—¶åºç»´åº¦å‹ç¼©ï¼š(Batch, 140, 64) -> (Batch, 64)
        feat = torch.mean(gcn_out, dim=1)
        
        # 3. BN å’Œ åˆ†ç±»
        feat = self.bn_final(feat)
        logits = self.classifier(feat)
        
        return logits

# ==========================================
# ç¬¬ä¸‰éƒ¨åˆ†: è®­ç»ƒæµç¨‹ (äº”æŠ˜ CV, ç§»é™¤æ—©åœ)
# ==========================================

def train_k_fold():
    # --- é…ç½® ---
    base_path = "./"
    fmri_dir = os.path.join(base_path, "datasets", "fMRI")
    smri_file = os.path.join(base_path, "datasets", "GMV_Node_Features.csv")
    label_file = os.path.join(base_path, "datasets", "labels.csv")
    adj_file = os.path.join(base_path, "datasets", "FC.csv")
    
    BATCH_SIZE = 16
    LEARNING_RATE = 0.001 
    NUM_EPOCHS = 80
    K_FOLDS = 5
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f" Device: {device} (Mode: GCN Only - No Early Stopping)")

    # 1. æ•°æ®å‡†å¤‡
    if not os.path.exists(adj_file):
        raise FileNotFoundError(f"Missing: {adj_file}")
    adj_static = load_adjacency_matrix(adj_file).to(device)
    
    full_dataset = MultimodalDataset(fmri_dir, smri_file, label_file)
    all_labels = np.array(full_dataset.labels_list)
    all_indices = np.arange(len(full_dataset))

    # 2. äº¤å‰éªŒè¯
    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)
    fold_results = []
    
    print(f"\nâš¡ å¼€å§‹ {K_FOLDS} æŠ˜äº¤å‰éªŒè¯ (GCN Only, Full Epochs)...")

    for fold, (train_idx, val_idx) in enumerate(skf.split(all_indices, all_labels)):
        print(f"\n=== Fold {fold+1}/{K_FOLDS} ===")
        
        train_subset = Subset(full_dataset, train_idx)
        val_subset = Subset(full_dataset, val_idx)
        
        # åŠ æƒé‡‡æ ·
        y_train = all_labels[train_idx]
        class_counts = np.bincount(y_train)
        class_weights = np.nan_to_num(1. / class_counts, posinf=0.0)
        sample_weights = class_weights[y_train]
        
        sampler = WeightedRandomSampler(
            weights=torch.from_numpy(sample_weights).float(),
            num_samples=len(train_idx),
            replacement=True
        )
        
        train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, sampler=sampler, drop_last=True)
        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)
        
        # åˆå§‹åŒ–æ¨¡å‹ (ä½¿ç”¨ GCN_Only_Model)
        model = GCN_Only_Model(n_nodes=116, n_classes=2).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)
        
        best_fold_bacc = 0.0
        best_fold_acc = 0.0
        
        # [ä¿®æ”¹] å¾ªç¯è·‘æ»¡ NUM_EPOCHSï¼Œä¸æå‰é€€å‡º
        for epoch in range(NUM_EPOCHS):
            model.train()
            running_loss = 0.0
            
            for fmri, smri, labels in train_loader:
                fmri, smri, labels = fmri.to(device), smri.to(device), labels.to(device)
                
                optimizer.zero_grad()
                outputs = model(fmri, smri, adj_static)
                loss = criterion(outputs, labels)
                loss.backward()
                
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                optimizer.step()
                running_loss += loss.item()

            # éªŒè¯
            model.eval()
            preds_list = []
            targets_list = []
            
            with torch.no_grad():
                for fmri, smri, labels in val_loader:
                    fmri, smri, labels = fmri.to(device), smri.to(device), labels.to(device)
                    outputs = model(fmri, smri, adj_static)
                    _, predicted = torch.max(outputs.data, 1)
                    
                    preds_list.extend(predicted.cpu().numpy())
                    targets_list.extend(labels.cpu().numpy())
            
            epoch_acc = np.mean(np.array(preds_list) == np.array(targets_list)) * 100
            epoch_bacc = balanced_accuracy_score(targets_list, preds_list) * 100
            
            # [ä¿®æ”¹] ä»…è®°å½•æœ€ä½³ï¼Œä¸ä¸­æ–­å¾ªç¯
            if epoch_bacc > best_fold_bacc:
                best_fold_bacc = epoch_bacc
                best_fold_acc = epoch_acc
                # ä¾ç„¶å¯ä»¥ä¿å­˜æœ€ä½³æƒé‡ï¼Œé˜²æ­¢è·‘è¿‡å¤´
                # torch.save(model.state_dict(), f"best_gcn_fold_{fold+1}.pth")
                print(f"  Epoch {epoch+1}: B-Acc {epoch_bacc:.2f}% ğŸ†™")
            
            # æ¯10è½®æ‰“å°ä¸€æ¬¡æ—¥å¿—
            if (epoch+1) % 10 == 0:
                print(f"  Epoch {epoch+1}: Loss {running_loss/len(train_loader):.4f} | Val B-Acc: {epoch_bacc:.2f}% (Best: {best_fold_bacc:.2f}%)")

        print(f" Fold {fold+1} å®Œæˆ. Best Balanced Acc: {best_fold_bacc:.2f}% (Acc: {best_fold_acc:.2f}%)")
        fold_results.append({'fold': fold+1, 'bacc': best_fold_bacc, 'acc': best_fold_acc})

    # æ±‡æ€»
    print("\n" + "="*35)
    print("   GCN-Only (No LSTM) Results   ")
    print("="*35)
    avg_bacc = sum([r['bacc'] for r in fold_results]) / K_FOLDS
    avg_acc = sum([r['acc'] for r in fold_results]) / K_FOLDS
    
    for res in fold_results:
        print(f"Fold {res['fold']}: Balanced Acc = {res['bacc']:.2f}% | Acc = {res['acc']:.2f}%")
        
    print("-" * 35)
    print(f"Avg Balanced Acc: {avg_bacc:.2f}%")
    print(f"Avg Accuracy    : {avg_acc:.2f}%")
    print("="*35)

if __name__ == "__main__":
    train_k_fold()